{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4da38ad",
   "metadata": {},
   "source": [
    "# 1. Libraries & Sample Data\n",
    "The first step is to load our Python Libraries and download the sample data. The dataset represents Apple stock price (1d bars) for the year 2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aa05430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Python Libraries\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa831031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>BB_upper</th>\n",
       "      <th>BB_lower</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009-04-30</th>\n",
       "      <td>4.493929</td>\n",
       "      <td>4.570118</td>\n",
       "      <td>4.055596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-05-01</th>\n",
       "      <td>4.544286</td>\n",
       "      <td>4.578017</td>\n",
       "      <td>4.099590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-05-04</th>\n",
       "      <td>4.716786</td>\n",
       "      <td>4.642685</td>\n",
       "      <td>4.092351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-05-05</th>\n",
       "      <td>4.716786</td>\n",
       "      <td>4.699974</td>\n",
       "      <td>4.083704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-05-06</th>\n",
       "      <td>4.732143</td>\n",
       "      <td>4.750376</td>\n",
       "      <td>4.083481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Close  BB_upper  BB_lower\n",
       "Date                                    \n",
       "2009-04-30  4.493929  4.570118  4.055596\n",
       "2009-05-01  4.544286  4.578017  4.099590\n",
       "2009-05-04  4.716786  4.642685  4.092351\n",
       "2009-05-05  4.716786  4.699974  4.083704\n",
       "2009-05-06  4.732143  4.750376  4.083481"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data, cleaned, not normalized, with features\n",
    "data = pd.read_csv(\"../datasets/AAPL_2009-2010_6m_features_1d.csv\")\n",
    "state_features = [\"Date\", \"Close\", \"BB_upper\", \"BB_lower\"]\n",
    "data = data[state_features]\n",
    "data.set_index(\"Date\", inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e8526a",
   "metadata": {},
   "source": [
    "# 2. Train / Test Split\n",
    "Now that we have loaded our cleaned price dataset, we are ready to feed the data into our model. With this in mind, we select Close as our singular training feature, and split the data ito train and test data (80/20 split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7557879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset df into train (80%) and test (20%) datasets\n",
    "train_df = data.sample(frac=0.8, random_state=42)\n",
    "test_df = data.drop(train_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1dca49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train df\n",
      "               Close  BB_upper  BB_lower\n",
      "Date                                    \n",
      "2009-09-23  6.625000  6.735803  5.710983\n",
      "2009-07-20  5.461071  5.419319  4.678538\n",
      "2009-07-21  5.411071  5.471442  4.676915\n",
      "2009-11-16  7.379643  7.522641  6.647538\n",
      "2009-12-11  6.952500  7.530241  6.692152\n",
      "Test df\n",
      "               Close  BB_upper  BB_lower\n",
      "Date                                    \n",
      "2009-05-01  4.544286  4.578017  4.099590\n",
      "2009-05-20  4.495357  4.763943  4.271236\n",
      "2009-05-29  4.850357  4.901817  4.249398\n",
      "2009-06-01  4.976786  4.968877  4.225587\n",
      "2009-06-23  4.786071  5.192462  4.702146\n"
     ]
    }
   ],
   "source": [
    "# display train and test dfs (ensure no overlap)\n",
    "print(\"Train df\")\n",
    "print(train_df.head())\n",
    "print(\"Test df\")\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92a6ed95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137, 3)\n"
     ]
    }
   ],
   "source": [
    "# convert train and test dfs to np arrays with dtype=float\n",
    "X_train = train_df.values.astype(float)\n",
    "X_test = test_df.values.astype(float)\n",
    "# print the shape of X_train to remind yourself how many examples and features are in the dataset\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714c77a8",
   "metadata": {},
   "source": [
    "# 3. Define the Agent\n",
    "Now that our data is ready to use, we can define the Reinforcement Learning Agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd06b6d",
   "metadata": {},
   "source": [
    "### Define the DQN Model\n",
    "The first step in defining our agent is the Deep Q-Network model definition. \n",
    "- we are creating a model sequential model with four layers. The first three layers have output shape of 64, 32, and 8, respectively, and a RELU activation. \n",
    "- The output layer has an output shape of the size of our action space (buy, sell, hold), and a linear activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1e98591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size=3, seed=42):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action (default=3 for buy, sell, hold)\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.Q = nn.Sequential(\n",
    "            nn.Linear(state_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, action_size),  # Linear activation by default\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        actions = self.Q(state)\n",
    "        return actions\n",
    "\n",
    "\n",
    "# test model\n",
    "q_net = QNetwork(state_size=8)  # action_size defaults to 3 (buy, sell, hold)\n",
    "# fake input, batch size 4\n",
    "states = torch.rand((4, 8))\n",
    "# fake output\n",
    "print(q_net(states).shape)  # Should print torch.Size([4, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d397f0f",
   "metadata": {},
   "source": [
    "### Define Experience Replay Buffer\n",
    "The Experience Replay Buffer is a key component of our agent implementation. It consists of a deque data structure with a fixed maximum length (buffer_size) that stores experiences as tuples of (state, action, reward, next_state, done). \n",
    "\n",
    "The buffer serves two main purposes:\n",
    "1. Storing and sampling experiences for training - Random batches of experiences are drawn from the buffer to train the agent\n",
    "2. Maintaining recent experiences - The last n experiences (where n is the batch size) are kept to calculate target Q-values for the current state during training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34436fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            batch_size (int): size of each training batch\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.experience = namedtuple(\n",
    "            \"Experience\",\n",
    "            field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"],\n",
    "        )\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def _to_tensor(self, data, dtype=torch.float):\n",
    "        \"\"\"Convert numpy array to tensor with specified dtype and device in one operation\"\"\"\n",
    "        return torch.from_numpy(np.vstack(data)).to(device=device, dtype=dtype)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        experiences = [e for e in experiences if e is not None]\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "\n",
    "        states = self._to_tensor(states)\n",
    "        actions = self._to_tensor(actions, dtype=torch.long)\n",
    "        rewards = self._to_tensor(rewards)\n",
    "        next_states = self._to_tensor(next_states)\n",
    "        dones = self._to_tensor(dones, dtype=torch.uint8)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d085318b",
   "metadata": {},
   "source": [
    "### Define Agent Class\n",
    "\n",
    "The Agent class implements a Deep Q-Learning agent that interacts with a trading environment. This implementation features a Double DQN (Deep Q-Network) architecture with experience replay for stable learning.\n",
    "\n",
    "#### Main Methods\n",
    "\n",
    "1. **step(state, action, reward, next_state, done)**\n",
    "   - Stores experience in replay buffer\n",
    "   - Triggers learning every `update_step` steps\n",
    "   - Returns loss value when learning occurs\n",
    "\n",
    "2. **select_action(state, epsilon)**\n",
    "   - Implements epsilon-greedy policy for action selection\n",
    "   - In test mode: Always selects best action\n",
    "   - In training mode: Balances exploration and exploitation\n",
    "\n",
    "3. **learn(experiences)**\n",
    "   - Updates Q-Network parameters using sampled experiences\n",
    "   - Implements Double DQN learning algorithm:\n",
    "     1. Computes TD targets using target network\n",
    "     2. Computes current Q-values using local network\n",
    "     3. Updates networks using MSE loss\n",
    "     4. Performs soft update of target network\n",
    "\n",
    "4. **Network Updates**\n",
    "   - `soft_update()`: Gradually updates target network (θ_target = α×θ + (1-α)×θ_target)\n",
    "   - `hard_update()`: Directly copies parameters from local to target network\n",
    "\n",
    "5. **Model Persistence**\n",
    "   - `save(filename)`: Saves model parameters to file\n",
    "   - `load(filename)`: Loads model parameters from file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4362197e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features,\n",
    "        window_size,\n",
    "        test_mode=False,\n",
    "        buffer_size=int(1e5),\n",
    "        batch_size=64,\n",
    "        gamma=0.99,\n",
    "        alpha=1e-3,\n",
    "        lr=5e-4,\n",
    "        update_step=4,\n",
    "        seed=42,\n",
    "    ):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "\n",
    "        Key Features:\n",
    "            - State space: Concatenated window of features (window_size * num_features)\n",
    "            - Action space: 3 discrete actions (0=hold, 1=buy, 2=sell)\n",
    "            - Experience replay: Stores and samples past experiences for stable learning\n",
    "            - Target network: Updated softly for stable training\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): Maximum size of experience replay buffer (default: 1e5)\n",
    "            batch_size (int): Size of each training batch (default: 64)\n",
    "            gamma (float): Discount factor for future rewards (default: 0.99)\n",
    "            alpha (float): Soft update interpolation parameter (default: 1e-3)\n",
    "            lr (float): Learning rate for optimizer (default: 5e-4)\n",
    "            update_step (int): Frequency of network updates (default: 4)\n",
    "            test_mode (bool): Flag for switching between training and testing behavior\n",
    "        \"\"\"\n",
    "        self.seed = random.seed(seed)\n",
    "        self.test_mode = test_mode\n",
    "\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.update_step = update_step\n",
    "\n",
    "        self.state_size = window_size * num_features\n",
    "        self.action_size = 3\n",
    "\n",
    "        # Q-Network\n",
    "        self.Q = QNetwork(self.state_size, self.action_size, seed).to(device)\n",
    "        self.Q_target = QNetwork(self.state_size, self.action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.Q.parameters(), lr=self.lr)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(self.buffer_size, self.batch_size, seed)\n",
    "        # Initialize time step (for updating every update step)\n",
    "        self.t_step = 0\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f\"Q Network Arch: {self.Q}\\n\"\n",
    "            f\"State space size: {self.state_size}\\n\"\n",
    "            f\"Action space size: {self.action_size}\\n\"\n",
    "            f\"Current Memory size: {len(self.memory)}\"\n",
    "        )\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn every update_step time steps.\n",
    "        self.t_step = (self.t_step + 1) % self.update_step\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                loss = self.learn(experiences)\n",
    "                return loss\n",
    "\n",
    "    def select_action(self, state, epsilon=0.0):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.Q.eval()\n",
    "        with torch.no_grad():\n",
    "            actions = self.Q(state)\n",
    "\n",
    "        if self.test_mode:\n",
    "            return np.argmax(actions.cpu().data.numpy())\n",
    "\n",
    "        self.Q.train()\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() <= epsilon:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "        else:\n",
    "            return np.argmax(actions.cpu().data.numpy())\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # get experiences\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # compute td targets using target network\n",
    "        with torch.no_grad():\n",
    "            Q_targets_next = torch.max(\n",
    "                self.Q_target(next_states), dim=-1, keepdim=True\n",
    "            )[0]\n",
    "            Q_targets = rewards + (1 - dones) * self.gamma * Q_targets_next\n",
    "\n",
    "        # compute curr values using local network\n",
    "        Q_expected = torch.gather(self.Q(states), dim=-1, index=actions)\n",
    "\n",
    "        # compute mean squared loss using td error\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # update local network parameters\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # update target network parameters\n",
    "        self.soft_update()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def soft_update(self):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = alpha*θ + (1 - alpha)*θ_target\n",
    "        =>\n",
    "        θ_target = θ_target + alpha*(θ - θ_target)\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            Q (PyTorch model): weights will be copied from\n",
    "            Q_target (PyTorch model): weights will be copied to\n",
    "            alpha (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(\n",
    "            self.Q_target.parameters(), self.Q.parameters()\n",
    "        ):\n",
    "            target_param.data.copy_(\n",
    "                target_param.data + self.alpha * (local_param.data - target_param.data)\n",
    "            )\n",
    "\n",
    "    def hard_update(self):\n",
    "        \"\"\"Hard update: θ_target = θ\"\"\"\n",
    "        for target_param, local_param in zip(\n",
    "            self.Q_target.parameters(), self.Q.parameters()\n",
    "        ):\n",
    "            target_param.data.copy_(local_param.data)\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"Save model parameters.\"\"\"\n",
    "        torch.save(self.Q.state_dict(), filename)\n",
    "\n",
    "    def load(self, filename):\n",
    "        \"\"\"Load model parameters.\"\"\"\n",
    "        checkpoint = torch.load(filename)\n",
    "        self.Q.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0f1502",
   "metadata": {},
   "source": [
    "#### Usage Modes\n",
    "\n",
    "1. **Training Mode** (`test_mode=False`)\n",
    "   - Enables exploration through epsilon-greedy policy\n",
    "   - Updates networks through learning process\n",
    "   - Stores experiences in replay buffer\n",
    "\n",
    "2. **Testing Mode** (`test_mode=True`)\n",
    "   - Disables exploration (always selects best action)\n",
    "   - No network updates\n",
    "   - Useful for evaluation and deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63b8c216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q Network Arch: QNetwork(\n",
      "  (Q): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=32, out_features=8, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=8, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "State space size: 3\n",
      "Action space size: 3\n",
      "Current Memory size: 0\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(num_features=X_train.shape[1], window_size=1)\n",
    "print(agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
